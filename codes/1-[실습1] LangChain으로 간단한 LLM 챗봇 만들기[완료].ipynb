{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rxt9bMf3orJ"
   },
   "source": [
    "# [실습1] LangChain으로 간단한 LLM 챗봇 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG_0zjG13zZy"
   },
   "source": [
    "## 실습 목표\n",
    "---\n",
    "- LangChain을 활용해서 Mistral 7B 모델을 사용하는 챗봇을 개발합니다.\n",
    "- 짧은 Chain을 구성하고, 이를 활용해서 챗봇을 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS7RvUWi5vJe"
   },
   "source": [
    "## 실습 목차\n",
    "---\n",
    "\n",
    "1. **ChatOllama Agent 생성:** 사용자의 입력에 대한 Mistral 7B 모델의 답변을 받아오는 Agent를 생성합니다.\n",
    "\n",
    "2. **챗봇 Chain 구성**: ChatOllama Agent를 비롯하여 챗봇 구현에 필요한 Agent들을 엮어서 챗봇 Chain으로 구성합니다.\n",
    "\n",
    "3. **챗봇 사용**: 여러분이 구성하신 챗봇을 사용해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-rxhtJI5_D2"
   },
   "source": [
    "## 실습 개요\n",
    "---\n",
    "\n",
    "LangChain의 Chain을 활용해서 Mistral 7B 모델을 활용하는 챗봇을 구현하고, Chain을 형성하는 방법을 이해합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라이브러리 설명\n",
    "\n",
    "langchain_core\n",
    "핵심 모듈로, Langchain의 기본 기능을 담당합니다.\n",
    "Langchain의 뼈대 역할을 하는 부분으로, 모델 호출, 체인 구성, 메모리 관리 등 주요 기능이 여기에 포함됩니다.\n",
    "쉽게 말해, Langchain 전체의 작동 원리를 제공하는 기반 코드를 담고 있는 라이브러리입니다.\n",
    "\n",
    "langchain_community\n",
    "커뮤니티 주도로 만들어진 확장 모듈입니다.\n",
    "Langchain이 성장하면서, 다양한 사람들이 기여한 플러그인, 템플릿, 커스터마이징된 기능 등이 포함된 공간이라고 보면 됩니다.\n",
    "즉, 커뮤니티 사용자들이 필요로 하는 기능이나 새로운 아이디어를 반영한 모듈들이 여기에 속합니다. 쉽게 말하면 Langchain의 확장팩 같은 개념입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama란?\n",
    "\n",
    "Ollama는 인공지능 기반의 챗봇 모델을 제공하는 플랫폼으로, ChatGPT와 유사하게 자연어 처리 기술을 활용해 \n",
    "대화형 인공지능 시스템을 구축하는 데 도움을 줍니다. Ollama는 다양한 언어 모델을 지원하고, \n",
    "이를 쉽게 활용할 수 있는 API를 제공합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 환경 설정\n",
    "- 필요한 라이브러리를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# HumanMessage: 대화형 상황에서 인간 사용자가 입력한 메시지를 나타냄\n",
    "# SystemMessage: 시스템 또는 모델에서 나온 메시지를 나타내며, 주로 지시사항이나 응답을 설정하는 역할을 함\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# StrOutputParser: 모델의 출력값을 받아서 문자열 형식으로 변환하는 파서\n",
    "# 모델의 응답을 일관성 있게 처리하고 구조화하는 데 유용함\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# ChatPromptTemplate: 모델에게 입력할 프롬프트(지시사항)를 어떻게 구성할지 정의하는 템플릿\n",
    "# 대화를 위한 지침이나 콘텐츠를 템플릿 형식으로 설정할 수 있게 해줌\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "# ChatOllama: Langchain에서 지원하는 Ollama라는 이름의 커뮤니티 기반 챗봇 모델\n",
    "# 이 모델은 주로 대화 상호작용을 위해 커스터마이즈되었으며, 커뮤니티에서 기여한 최적화된 모델임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ollama를 통해 Mistral 7B 모델을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 실습은 nohup ollama serve & 명령어를 통해 백그라운드에서 ollama serve 명령을 실행한 상태입니다.\n",
    "# 여러분이 현업에서 사용할 때에는 위 명령어를 통해 serve를 백그라운드에서 실행하는 것이 좋습니다.\n",
    "!ollama pull mistral:7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kOsIuLd6EA9"
   },
   "source": [
    "## 1. ChatOllama Agent 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral 7B 모델을 사용하는 ChatOllama Agent를 생성합니다. \n",
    "- ChatOllama Agent는 사용자의 입력을 Ollama를 통해 로컬에서 구동한 LLM에 전송하고, 그 답변을 반환합니다.\n",
    "- 본 RAG 과정에서는 LLM으로 ChatOllama와 오픈 소스 LLM을 활용할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Sw25tx4wW-sO"
   },
   "outputs": [],
   "source": [
    "# 먼저, mistral:7b 모델을 사용하는 ChatOllama 객체를 생성합니다.\n",
    "llm = ChatOllama(model=\"mistral:7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPEN AI API 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "OPENAI_API_KEY= 'sk'\n",
    "llm  = ChatOpenAI(model='gpt-4o-mini', openai_api_key=OPENAI_API_KEY)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent를 구성했으니, 이제 Agent를 사용해봅시다.\n",
    "\n",
    "### 1-1. Runnable interface\n",
    "\n",
    "LangChain에서 Chain으로 엮을 수 있는 대부분의 구성 요소 (Agent, Tool 등..)는 \"Runnable\" protocol을 공유합니다.\n",
    "- 관련 LangChain API 문서: [langchain_core.runnables.base.Runnable — 🦜🔗 LangChain 0.1.4](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable)\n",
    "\n",
    "Runnable protocol을 공유하는 구성 요소는 모두 아래 세 메서드를 가지고 있습니다:\n",
    "- stream: 구성 요소의 답변을 순차적으로 반환한다 (stream back)\n",
    "- invoke: 입력된 값으로 chain을 호출하고, 그 결과를 반환한다.\n",
    "- batch: 입력값 리스트 (batch)로 chain을 호출하고, 그 결과를 반환한다.\n",
    "\n",
    "예시로, 저희가 방금 사용한 `ChatOllama` Class는 \"Runnable\" 하기 때문에 `invoke` 메서드를 가지고 있습니다.\n",
    "- invoke() 메서드를 통해 Agent, Chain 등에 데이터를 입력하고, 그 출력을 받아올 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`invoke` 메서드를 사용해봅시다. 여기서는 \"당신은 누구입니까?\" 라는 질문을 입력하면 Agent가 OpenAI API를 통해 Mistral 7B 모델의 답변을 받아 출력할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "runnable이란?\n",
    "\n",
    "특정 작업(예: 텍스트 생성, 데이터 처리, API 호출 등)을 실행할 수 있는 객체입니다.\n",
    "이런 작업들은 독립적으로 실행될 수도 있고, 여러 작업을 연결하여 하나의 흐름으로 만들 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"당신은 누구입니까?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순 텍스트 뿐만 아니라, 시스템, 사람, AI의 답변을 리스트로 정리하여 입력할 수 있습니다. \n",
    "\n",
    "여기서는 LangChain의 `SystemMessage`, `HumanMessage` Class를 활용해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\"당신은 친절한 AI 어시스턴트 입니다.\"),\n",
    "    HumanMessage(\"한글로 당신을 소개해주세요.\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시스템 프롬프트에 '친절한 AI 어시스턴트' 라는 역할을 명시하였습니다.\n",
    "\n",
    "이제 Mistral 7B 모델이 아까와 같은 질문에 어떻게 답했는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 질문을 했음에도 자신을 소개하는 문구가 조금 달라진 것 을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TODO] 다양한 역할을 적용해서 어떻게 답변이 달라지는지 자유롭게 실험해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"유치원선생님\"\n",
    "messages = [\n",
    "    SystemMessage(f\"당신은 {role} 입니다.\"),\n",
    "    HumanMessage(\"당신을 소개해주세요.\"),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메타 데이터 예시\n",
    "response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 14, 'total_tokens': 61, 'prompt_tokens_details': {'cached_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini', 'system_fingerprint': 'fp_e2bde53e6e', 'finish_reason': 'stop', 'logprobs': None}, id='run-09b38547-c5b7-4945-9e9d-b8c673decd1b-0', usage_metadata={'input_tokens': 14, 'output_tokens': 47, 'total_tokens': 61}\n",
    "\n",
    "메타 데이터 설명\n",
    "token_usage: 모델이 사용한 토큰에 대한 정보입니다.\n",
    "completion_tokens: 모델이 생성한 응답에서 사용한 토큰 수입니다. 이 예시에서는 47개의 토큰을 사용했습니다.\n",
    "prompt_tokens: 입력 요청에서 사용된 토큰 수입니다. 이 경우 14개의 토큰이 사용되었습니다.\n",
    "total_tokens: 요청과 응답에서 사용된 총 토큰 수입니다. 여기서는 61개의 토큰이 사용되었습니다.\n",
    "prompt_tokens_details.cached_tokens: 이전 대화에서 캐시된 토큰 수를 나타냅니다. 이 예시에서는 캐시된 토큰이 없었습니다 (0).\n",
    "completion_tokens_details.reasoning_tokens: 모델이 추론할 때 사용한 토큰 수를 나타냅니다. 이 경우 0으로 설정되어 있습니다.\n",
    "model_name: 응답을 생성하는 데 사용된 모델의 이름입니다. 이 경우 gpt-4o-mini 모델이 사용되었습니다.\n",
    "system_fingerprint: 시스템 또는 모델의 고유 식별자입니다. 여기서는 fp_e2bde53e6e로 나타나 있습니다.\n",
    "finish_reason: 응답이 완료된 이유를 나타냅니다. 이 경우, 'stop'은 응답이 자연스럽게 완료되었음을 의미합니다.\n",
    "logprobs: 각 토큰에 대한 확률 값 로그가 기록되었는지 여부입니다. 이 경우에는 None으로 설정되어 있어, 이 정보를 기록하지 않았음을 나타냅니다.\n",
    "id: 요청에 대한 고유 식별자입니다. 여기서는 run-09b38547-c5b7-4945-9e9d-b8c673decd1b-0가 요청을 나타냅니다.\n",
    "usage_metadata: 입력 및 출력 토큰 사용량에 대한 메타 데이터입니다.\n",
    "input_tokens: 입력 요청에 사용된 토큰 수입니다. 여기서는 14개의 토큰이 사용되었습니다.\n",
    "output_tokens: 모델이 생성한 출력(응답)에서 사용된 토큰 수입니다. 여기서는 47개의 토큰이 사용되었습니다.\n",
    "total_tokens: 입력과 출력에서 사용된 전체 토큰 수로, 61개입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcAdeSLwcN61"
   },
   "source": [
    "## 2. 챗봇 Chain 구성\n",
    "\n",
    "조금 전 `llm` object의 반환 값을 확인해보면, 다른 챗봇을 쓸 때 처럼 답변만 출력된 것이 아니라 다양한 메타 데이터 까지 같이 출력된 것을 확인할 수 있습니다.\n",
    "\n",
    "저희가 ChatGPT를 쓸 때를 생각해보면, 챗봇에 이걸 그대로 출력하는건 좀 부자연스럽습니다.\n",
    "\n",
    "이를 방지하기 위해, 답변을 parsing하는 `StrOutputParser`를 활용해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Output Parser\n",
    "- ChatOllama Agent를 비롯하여 LLM 답변 중 content만 자동으로 추출하는 Tool인 `StrOutputParser`를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StrOutputParser`를 사용해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parser가 제대로 답변만을 리턴하는지 확인합니다.\n",
    "parsed_response = parser.invoke(response)\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response에서 의도한 대로 텍스트만 추출하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 간단한 체인 구성\n",
    "\n",
    "- 저희는 `ChatOllama` 를 통해 Mistral 7B 모델의 답변을 받았고, 그 받은 답변을 다시 `StrOutputParser`에 입력해서 답변만 추출하였습니다.\n",
    "- 이 과정을 Chain으로 엮어서 간략화 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe (|) 연산자를 통해 두 객체를 연결해서 하나의 체인으로 만들 수 있습니다.\n",
    "chain = llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 표현식에서 파이프 기호(|)는 두 구성 요소가 연결되어 있다는 것을 의미합니다. \n",
    "첫번째 요소의 출력이 두번째 요소의 입력으로 들어가는 식으로 연결됩니다.\n",
    "이렇게 파이프로 연결될 수 있는 구성요소를 runnable이라고 부릅니다.\n",
    "쉽게 말해 runnable은 입력을 받아 출력을 생성할 수 있는 ‘실행가능’한 요소라고 생각하시면 됩니다.\n",
    "\n",
    "출처 : https://elyire.github.io/posts/LCEL%EA%B3%BC-runnable-%EB%B8%94%EB%A1%9C%EA%B7%B8/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain 역시 \"Runnable\" 하므로, `invoke` 메서드를 통해 Chain의 각 구성요소의 `invoke` 메서드를 순차적으로 호출할 수 있습니다.\n",
    "\n",
    "이때 특정 객체의 `invoke` 반환값은 Chain 상에서 연결된 다음 객체의 `invoke` 메서드에 입력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 체인을 실행하면, 체인에 포함된 모든 객체가 순차적으로 실행되며, 마지막 객체의 결과가 반환됩니다.\n",
    "# 여기서는 llm 객체가 먼저 실행되고, 그 결과가 parser 객체에 전달됩니다.\n",
    "chained_response = chain.invoke(messages)\n",
    "print(chained_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "별도의 절차 없이 바로 답변만 생성되는 것을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 프롬프트 템플릿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 여러분의 챗봇에 프로그래밍 조수, 시장조사 요원, 그냥 친구 등 다양한 역할을 적용해야 하는 상황이라 가정합시다.\n",
    "\n",
    "이를 구현할 수 있는 방법은 여러가지가 있지만, 우선 가장 간단한 방법으로 시스템 프롬프트에 '당신은 {역할} 입니다' 를 입력해 보겠습니다.\n",
    "\n",
    "이 방법이 항상 잘 작동하는 것은 아니지만, 간단한 예시 정도는 구현할 수 있습니다.\n",
    "\n",
    "사용자의 입력을 받고, 그에 대응하는 답변을 하기 위해서는 사용자의 입력을 적용할 수 있는 프롬프트 템플릿을 적용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# role에는 \"AI 어시스턴트\"가, question에는 \"당신을 소개해주세요.\"가 들어갈 수 있습니다.\n",
    "# Note. 사용한 문자열이 f-string이 아닙니다. \n",
    "# 여기서 중괄호로 감싼 텍스트는 LangChain placeholder를 나타내는 문자열입니다\n",
    "messages_with_variables = [\n",
    "    (\"system\", \"당신은 {role} 입니다.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate는 LangChain에서 사용하는 클래스 중 하나로,\n",
    "# 사용자가 정의한 메시지를 기반으로 하나의 프롬프트를 생성하는 데 사용됩니다.\n",
    "# from_messages() 메서드는 여러 메시지를 받아 프롬프트로 변환하는 역할을 합니다.\n",
    "prompt = ChatPromptTemplate.from_messages(messages_with_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 저희가 정의했던 코드와 크게 두가지 차이점이 있습니다.\n",
    "- HumanMessage, SystemMessage 같은게 없고, 튜플에 역할과 프롬프트가 저장되어 있습니다\n",
    "- 프롬프트에 {question} 같은 placeholder가 있습니다.\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"당신은 친절한 AI 어시스턴트 입니다.\"),\n",
    "    HumanMessage(\"한글로 당신을 소개해주세요.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe (|) 연산자를 통해 여러 객체를 연결해서 하나의 체인으로 만들 수 있습니다.\n",
    "# 이 경우, prompt 객체를 통해 변수를 적용한 프롬프트가 생성되고, llm 객체를 통해 이 프롬프트를 실행하고, 마지막으로 parser 객체를 통해 결과를 파싱합니다.\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"role\": \"친절한 페어 프로그래머\", \"question\": \"당신을 소개해주세요.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 챗봇 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로, 여러분이 제작하신 챗봇을 한번 사용해 봅시다.\n",
    "\n",
    "1. 사용자의 입력을 받아 앞서 정의한 Chain을 실행하고, 그 결과를 반환하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 실습이므로 앞서 사용했던 변수를 그대로 함수의 파라미터로 설정했습니다. \n",
    "# 다음 실습 부터는 이를 좀 더 고도화 해 볼 것입니다.\n",
    "def simple_chat(role, question, chain):\n",
    "    result = chain.invoke({\"role\": role, \"question\": question})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = input(\"제 역할을 입력해주세요: \")\n",
    "while True:\n",
    "    question = input(\"질문을 입력해주세요 (역할을 바꾸고 싶다면 '역할 교체' 를 입력해주세요. 종료를 원하시면 '종료'를 입력해주세요.): \")\n",
    "    if question == \"역할 교체\":\n",
    "        role = input(\"역할을 입력해주세요: \")\n",
    "        continue\n",
    "    elif question == \"종료\":\n",
    "        break\n",
    "    else:\n",
    "        # chain = prompt | llm | parser\n",
    "        result = simple_chat(role, question, chain)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 경우, 입력한 역할에 맞춰 어느 정도 대답하는 것을 확인할 수 있습니다. <br>\n",
    "현재 챗봇은 다음 한계점이 있습니다.\n",
    "- 문서나 데이터 기반 추론이 불가능하다.\n",
    "- Chat History를 기억하지 못한다.\n",
    "\n",
    "이어지는 실습에서 두 한계를 개선하고, 시장 조사 문서 기반 QA 봇을 만들어 봅시다.\n",
    "\n",
    "또한, 지금 구성한 챗봇은 UI가 없고 단순 표준 입출력 만을 사용합니다. 본 과정을 다 이수하시면 Streamlit을 활용해 간단히 챗봇을 만들어 볼 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가 실습\n",
    "\n",
    "시스템 프롬프트를 수정하면서 챗봇이 한글로 답변하는 빈도를 높여보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "인공지능 프로젝트 템플릿",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
